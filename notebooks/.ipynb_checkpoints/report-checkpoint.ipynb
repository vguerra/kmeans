{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means clustering\n",
    "\n",
    "KMeans is a distance-based unsupervised clustering algorithm that groups a set of $N d$-dimensional objects into $K$ clusters of similar objects. Objects in the same cluster are similar and different from the ones found in any other given cluster. It is important to note that the selection of $K$ is specific to the problem at hand and that there is not magic formula to compute it. Eventhough datasets have grown lately, this algorithm can be scaled due to its iterative nature.\n",
    "\n",
    "![Example of K-Means](./images/Kmeans-example.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The algorithm\n",
    "\n",
    "The algorithm is given as input mainly two things:\n",
    "\n",
    "* A dataset with $N$ $d$-dimensional points.\n",
    "* The number of desired clusters, denoted by $K$.\n",
    "\n",
    "and operates as follows:\n",
    "\n",
    "* **Step 0**: Choose randomly, $K$ points out of the out of the $N$ points in the dataset. We call this set of points *centroids*.\n",
    "* **Step 1**: Assign each data point to the closest cluster by computing the squared distance between the point and each of the centroids.\n",
    "* **Step 2**: Compute the new centroids by taking the average of all data points that belong to each cluster.\n",
    "* **Step 3**: Check for convergence, meaning, that the cluster assigned to each of the datapoints has not changed. If convergence has not been reached, to back to **Step 1**.\n",
    "\n",
    "The ultimate goal of the algorithm is to minimize the following objective function:\n",
    "\n",
    "$$\n",
    "J(\\mu, z) = \\sum_{i=1}^{N} \\sum_{k=1}^{K}z_i^k||x^i - \\mu_i||^2\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "* $x_i$ is a $d$-dimensional point in our dataset.\n",
    "* $\\mu_k$ represents the centroid of cluster $k$.\n",
    "* $z_i^k$ is an indicator variable such that $z_i^k = 1$ if $x_i$ belongs to the cluster $k$, $z_i^k = 0$ otherwise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial implementation - PySpark\n",
    "\n",
    "[Source code](https://github.com/vguerra/kmeans/blob/master/kmeans-dario-x.py)\n",
    "\n",
    "An initial implementation of the algorithm has been provided and it's been done in `PySpark` and after some analysis the following flaws have been spotted:\n",
    "\n",
    "* Unnecesary broadcasting of number of elements in the dataset using `.count()`, which is an action on a RDD that will cause data transfer between the executors and the driver. This can be avoided as the count of elements is not needed for the implementation of the algorithm.\n",
    "```python\n",
    "nb_elem = sc.broadcast(data.count())\n",
    "```\n",
    "\n",
    "* At each iteration of the algorithm a `.cartisian` operation is performed to combine the points in the dataset with the centroids computed so far, there are a couple of issues with this:\n",
    "    * This operation can cause a shuffle which might affect negatively the performance.\n",
    "    * Increases the amount of data that would need to be sent to an executor.\n",
    " This operation can be avoided by taking the array of centroids and broadcast it to the executors as it's is is way less than the amount of points in the dataset.\n",
    " \n",
    "```python\n",
    "joined = data.cartesian(centroides)\n",
    "```\n",
    "\n",
    "* Once the cluster each point belongs to has been computed, another join is done between assignment information and the original dataset. Again, this is dangerous as it mostlikely will cause a shuffle. A better implementation can be found where this is not needed.\n",
    "\n",
    "```python\n",
    "assignment = min_dist.join(data)\n",
    "```\n",
    "\n",
    "* At each iteration of the algorithm, we need to compute the new centroids. For this operation we need to know the count of points assigned to each operation plus the total sum of each of the $d$-dimensions. This is computed inneficiantely because:\n",
    "    * A `reduceByKey()` transformation is used to compute the number of points assigned to each cluster. As not all keys might be found in each partition to complete the count, a shuffle might be needed.\n",
    "```python\n",
    "count = clusters.map(lambda x: (x[0],1)).reduceByKey(lambda x,y: x+y)\n",
    "```\n",
    "    * The issue highlighted in the previous point is found when computing the sum of each dimension for all points in a cluster, since a `reduceByKey()` is applied.\n",
    "```python\n",
    "count = clusters.map(lambda x: (x[0],1)).reduceByKey(lambda x,y: x+y)\n",
    "```\n",
    "\n",
    "* There are several ways of implementing the stop condition logic for K-Means. In this initial python implementation assignments from the previous iteration are kept so that we can compute the count of points that changed clusters. Current and previous assignments are joinned and a count action is performed to see how many switches there were. Again, we highlight the shuffle issue with `join` transformations.\n",
    "\n",
    "```python\n",
    "switch = prev_assignment.join(min_dist)\\\n",
    "        .filter(lambda x: x[1][0][0] != x[1][1][0])\\\n",
    "        .count()\n",
    "```\n",
    "\n",
    "* Lastly, the error computed at the end of the implementation is slightly different from the objective function already mentioned at the beginning of the document, which is not really an issue, but getting error calculation correctly is needed if we want to compare results with other implementations.\n",
    "```python\n",
    "error = sqrt(min_dist.map(lambda x: x[1][1]).reduce(lambda x,y: x + y))/nb_elem.value\n",
    "```\n",
    "\n",
    "* On the algorithmic part, the initialization phase where the first set of centroids are picked, the implementation can be improved. This implementation picks simply at random $K$ centroids to form the clusters. We will see in the coming sections improved ways of initializing the algorithm.\n",
    "\n",
    "* The solution is not flexible and requires code changes if we want to run the clustering algorithm on a different dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved implementation in PySpark\n",
    "\n",
    "Given all the faults listed in the previous section, we have implemented the following improvements for the K-Means algorithm.\n",
    "\n",
    "### The initialization phase\n",
    "\n",
    "As mentioned before, there are well known ways of improving the resulting clustering for K-Means, meaning, minimizing further the objective function.\n",
    "\n",
    "It is well known that the algorithm running time can be exponential in the worst case and final solutions can be far away from the optimal one. This two issues are tightly coupled with the way initialization is performed, herefore recent work has focused on improving this phase.\n",
    "\n",
    "#### K-Means ++\n",
    "\n",
    "This algorithm selects from the dataset only the 1st centroid uniformly at random. The subsequent centroids are selected with a probability proportional to their contribution to the overall error. The idea behind this is that a good clustering is relatively spread out, so those points further away from previously selected centroids are given preference as centroids are picked.\n",
    "\n",
    "K-Means++ initialization leads to an $O(log k)$ approximation of the optimum clustering. If dataset is well clusterable the aproximation is constant.\n",
    "\n",
    "#### K-Means ||\n",
    "\n",
    "K-Means++ initialization has one disadvantage when working with considerable big datasets, and that is due to its sequential way of operating. Initial set of centroids are picked one after the other and a pass over the dataset is needed in order to compute errors ( which determine the distribution used to pick the next centroid ).\n",
    "\n",
    "K-Means || tries to imporve this situation. K-Means || in a nutshell is a parallel version of the K-Means++. The idea is to perform the K-Means initialization as follows:\n",
    "\n",
    "* Sample $O(k)$ points each step.\n",
    "* Repeat the process for $O(log n)$ rounds.\n",
    "* Then recluster all $O(k log n)$ obtained into $k$ initial centroids.\n",
    "\n",
    "### Agregation of values\n",
    "\n",
    "Given that the algorithm is computed in an incremental fashion, aggregation of values needs to be done efficiently ( without too much overhead of computation and as less network traffic as possible ). To speed up this we did the following:\n",
    "\n",
    "* [Broadcast the array](https://github.com/vguerra/kmeans/blob/master/kmeans-vguerra.py#L238) of centroids that should be used to compute distances. This is a good choice given that the number of centroids is considerably smaller than the amount of points in the dataset.\n",
    "\n",
    "* For [each of the partitions we run computation of errors and accumulators](https://github.com/vguerra/kmeans/blob/master/kmeans-vguerra.py#L243) that serve later to compute the new centroids, this has the nice feature of doing just one pass over the data for each iteration. The numbers computed are then reducedByKey, which might cause a shouffle but the amount of data that could be transmitted in this case is much less that the points in the dataset ( it is actually proportional to the number of centroids used for clustering ).\n",
    "\n",
    "### The stop condition\n",
    "\n",
    "The stop condition is based on two pieces of information: the limit of iterations or whether or not we reached convergence. \n",
    "\n",
    "To check for convergence you would need to check the assignments from the previous interation and check if at least one point changed cluster. This implies that you need to keep state of assignments from the previous iteration, which can be expensive if your dataset is big.\n",
    "\n",
    "Instead, we have [implemented the stop condition](https://github.com/vguerra/kmeans/blob/master/kmeans-vguerra.py#L263) in a way that does not require keeping state. We simply check if the resulting centroids for a given iteration change w.r.t to the centroids from the previous iteration.\n",
    "\n",
    "\n",
    "### PySpark implementation.\n",
    "\n",
    "[Source code](https://github.com/vguerra/kmeans/blob/master/kmeans-vguerra.py)\n",
    "\n",
    "\n",
    "The `PySpark` implementation presented in this project allows two different flavors of K-Means that differ on the initialization method used:\n",
    "\n",
    "* **[Random Initialization](https://github.com/vguerra/kmeans/blob/master/kmeans-vguerra.py#L84)**: This is the defacto initialization method used in K-Means implementations, choosing $K$ initial centroids uniformly random.\n",
    "\n",
    "* **[K-Means Parallel](https://github.com/vguerra/kmeans/blob/master/kmeans-vguerra.py#L119)**: Thie algorithm boils down to the parallel initialization method to choose the initial centroids. As mentioned in previous sections, there is a re-clustering step during the initialization, this re-clustering is implemented by running a *K-means++* locally on the driver.\n",
    "\n",
    "The K-Means with parallel initialization includes a phase of reclustering which has been implemented \n",
    "\n",
    "#### Comparing PySpark implementations.\n",
    "\n",
    "Now we compare the initial implementation handed with the project description and our PySpark implementation. The comparission is done using the Iris dataset. Here the results:\n",
    "\n",
    "|              | Initial version | Improved version |\n",
    "|--------------|:---------------:|-----------------:|\n",
    "| Algorithm    | K-Means         | K-Means parallel |\n",
    "| Error        | 97.3462         | 97.3259          |\n",
    "| Iterations   | 10              | 6                |\n",
    "| Running time | 27.6 s.         | 12.7 s.          |\n",
    "\n",
    "The improved version reduces the error in almost half of the steps and this is achieved obviously in less than half of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Migrating from PySpark to Scala\n",
    "\n",
    "The previous `PySpark` implementation has been migrated to `scala` using different APIs:\n",
    "\n",
    "* RDD ([source code](https://github.com/vguerra/kmeans/blob/master/scala/src/main/scala/KMeansRDD.scala#L121))\n",
    "* Dataframe ([source code](https://github.com/vguerra/kmeans/blob/master/scala/src/main/scala/KMeansDataFrame.scala#L96))\n",
    "* Dataset ([source code](https://github.com/vguerra/kmeans/blob/master/scala/src/main/scala/KMeansDataset.scala#L190))\n",
    "\n",
    "We provide a short description of the different implementations.\n",
    "\n",
    "### RDD Implementation\n",
    "\n",
    "The migration to `scala` from `PySpark` was relatively straight forward, as the API used in the `PySpark` version is very similar to `RDD`'s API.\n",
    "\n",
    "### Dataframe Implementation\n",
    "\n",
    "For the Dataframe implementation the challenge was to convert some of our functions to be compatible with the Spark SQL API.\n",
    "\n",
    "Some functionality needed to be converted to custom aggregation functions. For this, we needed to extend the `UserDefinedAggregateFunction` class and create our [`SumCoordinates`](https://github.com/vguerra/kmeans/blob/master/scala/src/main/scala/KMeansDataFrame.scala#L14) aggregation function.\n",
    "\n",
    "As well, some custom `udf`s so that we could make computations that could be applied to an entire `Column` in the `Dataframe`. For instance, a `udf` that would [compare previous and current error](https://github.com/vguerra/kmeans/blob/master/scala/src/main/scala/KMeansDataFrame.scala#L279-L285).\n",
    "\n",
    "### Dataset Implementation\n",
    "\n",
    "Since a to be able to load our datasets into a `Dataset` type in Spark you need to have a `Schema`, we had to define the schemas for all the datasets we wanted to benchmark with.\n",
    "\n",
    "Therefore we defined schemas for [`Iris`](https://github.com/vguerra/kmeans/blob/master/scala/src/main/scala/KMeansDataset.scala#L47) [`Spam`](https://github.com/vguerra/kmeans/blob/master/scala/src/main/scala/KMeansDataset.scala#L55) and [`Syntetic 5000`](https://github.com/vguerra/kmeans/blob/master/scala/src/main/scala/KMeansDataset.scala#L117) datasets.\n",
    "\n",
    "In order to have our algorithm work with all the `Schemas` above, we needed to change the functions to be [generic](https://github.com/vguerra/kmeans/blob/master/scala/src/main/scala/KMeansDataset.scala#L215-L223). Hence we defined a [common abstract class](https://github.com/vguerra/kmeans/blob/master/scala/src/main/scala/KMeansDataset.scala#L139-L143) for the custom types representing datasets could inherit from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "We ran experiments using 3 different datasets:\n",
    "\n",
    "* [Iris](https://github.com/vguerra/kmeans/blob/master/input/iris.data.txt): With feature vectors of 4 columns and 150 points.\n",
    "* [Spam](https://github.com/vguerra/kmeans/blob/master/input/spambase.data.txt): With feature vectors of 57 columns and 4601 points.\n",
    "* [Synthetic](https://github.com/vguerra/kmeans/blob/master/input/synthetic-5000-15.data.txt): With feature vectors of 15 columns and 5000 points. This is a synthetic dataset generated with isotropic Gussian blogs generator of `scikit-learn`. ([source code](https://github.com/vguerra/kmeans/blob/master/gen-data-set.py))\n",
    "\n",
    "Here are the results comparing all implementations: `PySpark`, `Scala-RDD`, `Scala-Dataframe`, `Scala-Dataset`. ( Results for best run out of 5 ).\n",
    "\n",
    "### Iris dataset\n",
    "\n",
    "With K = 3.\n",
    "\n",
    "|                                   | Best error | Iterations |\n",
    "|-----------------------------------|:----------:|-----------:|\n",
    "| Python - random init              | 97.3259    | 6          |\n",
    "| Python - parallel init            | 97.3462    | 3          |\n",
    "| Scala-Rdd - random init           | 97.3259    | 5          |\n",
    "| Scala-Rdd - parallel init         | 97.3462    | 5          |\n",
    "| Scala-dataset - random init       | 97.3462    | 12         |\n",
    "| **Scala-dataset - parallel init** | **97.3462**| **2**      |\n",
    "| Scala-dataframe - random init     | 97.3462    | 14         |\n",
    "| Scala-dataframe - parallel init   | 97.3462    | 2          |\n",
    "\n",
    "\n",
    "We see that the dataset implementation w/parallel initialization achieves good clustering in only 2 steps.\n",
    "\n",
    "#### Spam dataset\n",
    "\n",
    "With K = 30.\n",
    "\n",
    "|                                   |  Best error  | Iterations |\n",
    "|-----------------------------------|:------------:|-----------:|\n",
    "| Python - random init              | 218007.4591  | 14         |\n",
    "| Python - parallel init            | 198670.5992  | 4          |\n",
    "| Scala-Rdd - random init           | 171111.73    | 62         |\n",
    "| Scala-Rdd - parallel init         | 231058.0627  | 3          |\n",
    "| Scala-dataset - random init       | 178050.78    | 65         |\n",
    "| **Scala-dataset - parallel init** | **140632.0** | **7**      |\n",
    "| Scala-dataframe - random init     | 174026.81    | 44         |\n",
    "| Scala-dataframe - parallel init   | 150223.22    | 5          |\n",
    "\n",
    "Again, our dataset implementation w/parallel initialization achieves better error, at the cost of a couple iterations more.\n",
    "\n",
    "\n",
    "#### Synthetic 5000\n",
    "\n",
    "With K = 10.\n",
    "\n",
    "|                                 |  Best error | Iterations |\n",
    "|---------------------------------|:-----------:|-----------:|\n",
    "| Python - random init            | 18506.83394 | 7          |\n",
    "| Python - parallel init          | 18631.37890 | 9          |\n",
    "| Scala-Rdd - random init         | 18662.746   | 3          |\n",
    "| Scala-Rdd - parallel init       | 19008.086   | 1          |\n",
    "| Scala-dataset - random init     | 18619.31    | 3          |\n",
    "| Scala-dataset - parallel init   | 19031.938   | 1          |\n",
    "| Scala-dataframe - random init   | 18751.445   | 2          |\n",
    "| Scala-dataframe - parallel init | 19031.545   | 1          |\n",
    "\n",
    "In this case it is interesting to highlight that all flavors of the implementations that have *parallel initialization* converge in 1 step, which is quite nice, with not so much difference on the error. But let's remember that this is a dataset produced artificially, so an almost perfect clustering is expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this project, we took an initial implementation of the K-Means algorithm written in `PySpark` and analyzed it to propose a better implementation that reduces passes over the data and amount of data that need to be transfered between the driver and it's executors. As well, we implemented the initialization phase of K-Means ||, which is a state of the art algorithm to scale K-Means.\n",
    "\n",
    "Based on an imporved `PySpark` implementation we migrated the code base to the Scala RDD API, Dataframe API and Dataset API, overcoming the challenges of adapting functionality to be able to run using such APIs, like defining UDFs and custom Shemas for the datasets used to benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project layout\n",
    "\n",
    "Source code on [GitHub](https://github.com/vguerra/kmeans).\n",
    "\n",
    "* [`input`](https://github.com/vguerra/kmeans/tree/master/input): contains all datasets in `csv` format.\n",
    "* [`scala`](https://github.com/vguerra/kmeans/tree/master/scala): Contains the entire scala code base.\n",
    "    * [`src/main/KMeansApp.scala`](https://github.com/vguerra/kmeans/blob/master/scala/src/main/scala/KMeansApp.scala): Entry point for the different flavors of the Scala implementation.\n",
    "    * [`KMeansRDD.scala`](https://github.com/vguerra/kmeans/blob/master/scala/src/main/scala/KMeansRDD.scala): K-Means implementation with Spark RDD API.\n",
    "    * [`KMeansDataset.scala`](https://github.com/vguerra/kmeans/blob/master/scala/src/main/scala/KMeansDataset.scala): K-Means implementation with Spark Dataset API.\n",
    "    * [`KMeansDataFrame.scala`](https://github.com/vguerra/kmeans/blob/master/scala/src/main/scala/KMeansDataFrame.scala): K-Means implementation with Spark DataFrame API.\n",
    "* [`gen-data-set.py`](https://github.com/vguerra/kmeans/blob/master/gen-data-set.py): Support python file to generate synthetic dataset.\n",
    "* [`kmeans-dario-x.py`](https://github.com/vguerra/kmeans/blob/master/kmeans-dario-x.py): Initial version of `K-Means`.\n",
    "* [`kmeans-vguerra.py`](https://github.com/vguerra/kmeans/blob/master/kmeans-vguerra.py): `PySpark` implementation of K-Means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "All references used in the report ( unsorted ):\n",
    "\n",
    "* [Data Algorithms](http://shop.oreilly.com/product/0636920033950.do), O'Reilly Media.\n",
    "* [Introduction to Image Segmentation with K-Means clustering](https://towardsdatascience.com/introduction-to-image-segmentation-with-k-means-clustering-83fd0a9e2fc3), by Nagesh Singh Chauhan on Medium.\n",
    "* [K-Means, EM, Gaussian Mixture, Graph Theory](https://www.di.ens.fr/~fbach/courses/fall2013/lecture3.pdf), Lecture 3, by Francis Bach.\n",
    "* [Spark API Documentation](https://spark.apache.org/docs/latest/api.html).\n",
    "* [k-menas++: The Advantages of Careful Seeding](https://github.com/vguerra/kmeans/blob/master/kmeans-plusplus.pdf) paper.\n",
    "* [Scalable K-Means++](https://github.com/vguerra/kmeans/blob/master/kmeans-parallel-init.pdf) paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
